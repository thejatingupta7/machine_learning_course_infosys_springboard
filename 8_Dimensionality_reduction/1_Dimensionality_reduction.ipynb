{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de40a76-c061-4608-81f9-a9664000120a",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03527b9b-27c8-407b-8dac-3d91ee62eafb",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "  * Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.\n",
    "  * [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "  * Statistical procedure that utilise [orthogonal transformation](https://en.wikipedia.org/wiki/Orthogonal_transformation) technology\n",
    "  * Convert possible correlated features (predictors) into linearly uncorrelated features (predictors) called **principal components**\n",
    "  * \\# of principal components <= number of features (predictors)\n",
    "  * First principal component explains the largest possible variance\n",
    "  * Each subsequent component has the highest variance subject to the restriction that it must be orthogonal to the preceding components. \n",
    "  * A collection of the components are called vectors.\n",
    "  * Sensitive to scaling\n",
    "  * [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html): Component axes that maximise the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa47a4-427d-436b-b2c9-085c5537f399",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7e483-5943-463b-b129-d2b18d986169",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA) \n",
    "  * [Wikipedia](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\n",
    "  * [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html)\n",
    "  * Most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. \n",
    "  * Goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (“curse of dimensionality”) and also reduce computational costs.\n",
    "  * Locate the 'boundaries' around clusters of classes.  \n",
    "  * Projects data points on a line\n",
    "  * A centroid will be allocated to each cluster or have a centroid nearby\n",
    "  * [Sebastian Raschka](http://sebastianraschka.com/Articles/2014_python_lda.html): Maximising the component axes for class-separation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a85532-0c6b-4e2e-9fdd-9c566fc29096",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e437c-d982-44a3-ab46-f7852530a4c0",
   "metadata": {},
   "source": [
    "### Other Dimensionality Reduction Techniques\n",
    "\n",
    "* [Multidimensional Scaling (MDS) ](http://scikit-learn.org/stable/modules/manifold.html#multi-dimensional-scaling-mds)\n",
    "  * Seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
    "\n",
    "\n",
    "* [Isomap (Isometric Mapping)](http://scikit-learn.org/stable/modules/manifold.html#isomap)\n",
    "\n",
    "  * Seeks a lower-dimensional embedding which maintains geodesic distances between all points.\n",
    "\n",
    "\n",
    "* [t-distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
    "\n",
    "  * Nonlinear dimensionality reduction technique that is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot. \n",
    "  * Models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points. dimensional space (e.g., to visualize the MNIST images in 2D).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e418d78-903d-4f6a-b86f-84e6376af427",
   "metadata": {},
   "source": [
    "***\n",
    "# Gentle introduction to Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9c246-d917-4ffe-bba1-2f3b0290f5da",
   "metadata": {},
   "source": [
    "Linear Algebra revision:\n",
    "\n",
    "$$A=\\begin{bmatrix} 1. & 2. \\\\ 10. & 20. \\end{bmatrix}$$\n",
    "\n",
    "$$B=\\begin{bmatrix} 1. & 2. \\\\ 100. & 200. \\end{bmatrix}$$\n",
    "\n",
    "\\begin{align}\n",
    "A \\times B & = \\begin{bmatrix} 1. & 2. \\\\ 10. & 20. \\end{bmatrix} \\times \\begin{bmatrix} 1. & 2. \\\\ 100. & 200. \\end{bmatrix} \\\\\n",
    "& = \\begin{bmatrix} 201. & 402. \\\\ 2010. & 4020. \\end{bmatrix} \\\\\n",
    "\\end{align}\n",
    "\n",
    "By parts:\n",
    "$$A \\times B = \\begin{bmatrix} 1. \\times 1. + 2.  \\times 100. &  1. \\times 2. + 2. \\times 200. \\\\ \n",
    "10. \\times 1. + 20. \\times 100. & 10. \\times 2. + 20. \\times 200. \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a8607-8936-4dc2-9a37-68319726f370",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4de13f8-d979-4800-b24a-927fffeae828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ffa2a2a-2273-4c1e-8a3f-3d087b10ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[1., 2.], [10., 20.]]\n",
    "B = [[1., 2.], [100., 200.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b8584b-b2b2-40b4-8632-de7c94759589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0], [10.0, 20.0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8c6e659-b477-47ab-b8c9-3bdae7f518d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0], [100.0, 200.0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02fa94c-635c-4789-a054-e74bb82430f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 201.,  402.],\n",
       "       [2010., 4020.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31654a3d-7a4d-4a5f-aadb-7e5cbacadd7d",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
